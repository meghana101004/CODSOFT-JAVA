{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkIF-qKfOvFl"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNc5B1-pOvFn"
      },
      "source": [
        "# Use Gen AI Evaluation SDK to Evaluate Models in Vertex AI Studio, Model Garden, and Model Registry\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/evaluate_models_in_vertex_ai_studio_and_model_garden.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fevaluation%2Fevaluate_models_in_vertex_ai_studio_and_model_garden.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/evaluate_models_in_vertex_ai_studio_and_model_garden.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://upload.wikimedia.org/wikipedia/commons/9/91/Octicons-mark-github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/evaluation/evaluate_models_in_vertex_ai_studio_and_model_garden.ipynb\">\n",
        "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>    \n",
        "</table>\n",
        "\n",
        "<div style=\"clear: both;\"></div>\n",
        "\n",
        "<b>Share to:</b>\n",
        "\n",
        "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/evaluate_models_in_vertex_ai_studio_and_model_garden.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/evaluate_models_in_vertex_ai_studio_and_model_garden.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/evaluate_models_in_vertex_ai_studio_and_model_garden.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/53/X_logo_2023_original.svg\" alt=\"X logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/evaluate_models_in_vertex_ai_studio_and_model_garden.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
        "</a>\n",
        "\n",
        "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/evaluate_models_in_vertex_ai_studio_and_model_garden.ipynb\" target=\"_blank\">\n",
        "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
        "</a>            "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPgYxQc1OvFn"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "|Author(s) | [Jason Dai](https://github.com/jsondai) |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ_7WkHE3gNO"
      },
      "source": [
        "This notebook demonstrates how to get started with using the *Vertex AI Python SDK for Gen AI Evaluation Service* for generative models in Vertex AI Studio, Model Garden, and Model Registry.\n",
        "\n",
        "Gen AI Evaluation Service empowers you to comprehensively assess and enhance your generative AI models and applications. Whether you're selecting the ideal model, optimizing prompt templates, or evaluating fine-tuned checkpoints, this service provides the tools and insights you need.\n",
        "\n",
        "In this Colab tutorial, we'll explore three major use cases:\n",
        "\n",
        "1.  Run Evaluation on 1P Models\n",
        "  *   Learn how to evaluate `Gemini` models in Vertex AI Studio using the *Gen AI Evaluation Service SDK*.\n",
        "\n",
        "  *   Explore different evaluation metrics and techniques for assessing performance on various tasks.\n",
        "\n",
        "  *   Discover how to leverage the SDK for in-depth analysis and comparison of `Gemini` model variants.\n",
        "\n",
        "\n",
        "2.  Run Evaluation on 3P Models\n",
        "  *   Learn how to evaluate third-party open models, such as a pretrained `Llama 3.1` model, or a fine-tuned `Llama 3` model deployed in Vertex Model Garden, using the *Gen AI Evaluation Service SDK*.\n",
        "\n",
        "  *   Learn how to evaluate third-party closed model APIs, such as Anthropic's `Claude 3.5 Sonnet` model hosted on Vertex AI, using the *Gen AI Evaluation Service SDK*.\n",
        "\n",
        "  *   Gain insights into conducting controlled experiments by maintaining the same `EvalTask` configuration with fixed dataset and evaluation metrics while evaluating various model architectures and capabilities.\n",
        "\n",
        "\n",
        "3.  Prompt Engineering\n",
        "\n",
        "  *   Explore the impact of prompt design on model performance.\n",
        "  *   Utilize the SDK to systematically evaluate and refine your prompts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHGZmbkw6GgM"
      },
      "source": [
        "For additional use cases and advanced features, refer to our public documentation and notebook tutorials for evaluation use cases:\n",
        "\n",
        "* https://cloud.google.com/vertex-ai/generative-ai/docs/models/evaluation-overview#notebooks_for_evaluation_use_cases\n",
        "\n",
        "* https://cloud.google.com/vertex-ai/generative-ai/docs/models/run-evaluation\n",
        "\n",
        "Let's get started!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mN5IHo-aOvFo"
      },
      "source": [
        "**_NOTE_**: This notebook has been tested in the following environment:\n",
        "\n",
        "* Python version = 3.10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XZf_4VEOvFo"
      },
      "source": [
        "## Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE20na1OOvFo"
      },
      "source": [
        "### Install Vertex AI SDK for Gen AI Evaluation Service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "abLuRgBzOvFp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dda8b04-d21c-4c8f-c7e0-c084fb339411"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -U -q google-cloud-aiplatform[evaluation]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYJVHBVSZgTX"
      },
      "source": [
        "### Install other required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "squkG3h9ZZs8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45ab488c-3891-462d-9d72-6b24ed81554c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m317.4/480.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.8/222.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.8/464.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -U -q datasets\n",
        "%pip install -U -q anthropic[vertex]\n",
        "%pip install -U -q openai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe2lLnYuOvFp"
      },
      "source": [
        "### Restart runtime\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which restarts the current kernel.\n",
        "\n",
        "The restart might take a minute or longer. After it's restarted, continue to the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B3FDZs3qOvFp"
      },
      "outputs": [],
      "source": [
        "# import IPython\n",
        "\n",
        "# app = IPython.Application.instance()\n",
        "# app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqDc-oyiOvFp"
      },
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>⚠️ The kernel is going to restart. Wait until it's finished before continuing to the next step. ⚠️</b>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1oLkh17OvFp"
      },
      "source": [
        "### Authenticate your notebook environment (Colab only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9ygOCeYoOvFp"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wyNclIAOvFp"
      },
      "source": [
        "### Set Google Cloud project information and initialize Vertex AI SDK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GTL_YzF9OvFq"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"qwiklabs-gcp-04-ab5c423b76d5\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "if not PROJECT_ID or PROJECT_ID == \"[your-project-id]\":\n",
        "    raise ValueError(\"Please set your PROJECT_ID\")\n",
        "\n",
        "\n",
        "import vertexai\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBQgjn5wOvFq"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-TmyCxUSOvFq"
      },
      "outputs": [],
      "source": [
        "from anthropic import AnthropicVertex\n",
        "from google.auth import default, transport\n",
        "import openai\n",
        "from vertexai.evaluation import (\n",
        "    EvalTask,\n",
        "    MetricPromptTemplateExamples,\n",
        "    PairwiseMetric,\n",
        "    PointwiseMetric,\n",
        "    PointwiseMetricPromptTemplate,\n",
        ")\n",
        "from vertexai.generative_models import GenerativeModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfQ7sPtOjZOw"
      },
      "source": [
        "### Library settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RjWUgU1TjZOw"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "import logging\n",
        "import warnings\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.ERROR)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_Gw6YLeOvFq"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8imb3UdOvFq"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "\n",
        "import random\n",
        "import string\n",
        "\n",
        "from IPython.display import HTML, Markdown, display\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "def display_explanations(eval_result, metrics=None, n=1):\n",
        "    \"\"\"Display the explanations.\"\"\"\n",
        "    style = \"white-space: pre-wrap; width: 1500px; overflow-x: auto;\"\n",
        "    metrics_table = eval_result.metrics_table\n",
        "    df = metrics_table.sample(n=n)\n",
        "\n",
        "    if metrics:\n",
        "        df = df.filter(\n",
        "            [\"response\", \"baseline_model_response\"]\n",
        "            + [\n",
        "                metric\n",
        "                for metric in df.columns\n",
        "                if any(selected_metric in metric for selected_metric in metrics)\n",
        "            ]\n",
        "        )\n",
        "    for index, row in df.iterrows():\n",
        "        for col in df.columns:\n",
        "            display(HTML(f\"<div style='{style}'><h4>{col}:</h4>{row[col]}</div>\"))\n",
        "        display(HTML(\"<hr>\"))\n",
        "\n",
        "\n",
        "def display_eval_result(eval_result, title=None, metrics=None):\n",
        "    \"\"\"Display the evaluation results.\"\"\"\n",
        "    summary_metrics, metrics_table = (\n",
        "        eval_result.summary_metrics,\n",
        "        eval_result.metrics_table,\n",
        "    )\n",
        "\n",
        "    metrics_df = pd.DataFrame.from_dict(summary_metrics, orient=\"index\").T\n",
        "    if metrics:\n",
        "        metrics_df = metrics_df.filter(\n",
        "            [\n",
        "                metric\n",
        "                for metric in metrics_df.columns\n",
        "                if any(selected_metric in metric for selected_metric in metrics)\n",
        "            ]\n",
        "        )\n",
        "        metrics_table = metrics_table.filter(\n",
        "            [\n",
        "                metric\n",
        "                for metric in metrics_table.columns\n",
        "                if any(selected_metric in metric for selected_metric in metrics)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    if title:\n",
        "        # Display the title with Markdown for emphasis\n",
        "        display(Markdown(f\"## {title}\"))\n",
        "    # Display the summary metrics DataFrame\n",
        "    display(Markdown(\"### Summary Metrics\"))\n",
        "    display(metrics_df)\n",
        "    # Display the metrics table DataFrame\n",
        "    display(Markdown(\"### Row-based Metrics\"))\n",
        "    display(metrics_table)\n",
        "\n",
        "\n",
        "def display_radar_plot(eval_results, metrics=None):\n",
        "    \"\"\"Plot the radar plot.\"\"\"\n",
        "    fig = go.Figure()\n",
        "    for item in eval_results:\n",
        "        title, eval_result = item\n",
        "        summary_metrics = eval_result.summary_metrics\n",
        "        if metrics:\n",
        "            summary_metrics = {\n",
        "                k.replace(\"/mean\", \"\"): summary_metrics[k]\n",
        "                for k, v in summary_metrics.items()\n",
        "                if any(selected_metric + \"/mean\" in k for selected_metric in metrics)\n",
        "            }\n",
        "        fig.add_trace(\n",
        "            go.Scatterpolar(\n",
        "                r=list(summary_metrics.values()),\n",
        "                theta=list(summary_metrics.keys()),\n",
        "                fill=\"toself\",\n",
        "                name=title,\n",
        "            )\n",
        "        )\n",
        "    fig.update_layout(\n",
        "        polar=dict(radialaxis=dict(visible=True, range=[0, 5])), showlegend=True\n",
        "    )\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "def display_bar_plot(eval_results_list, metrics=None):\n",
        "    \"\"\"Plot the bar plot.\"\"\"\n",
        "    fig = go.Figure()\n",
        "    data = []\n",
        "\n",
        "    for eval_results in eval_results_list:\n",
        "        title, eval_result = eval_results[0], eval_results[1]\n",
        "\n",
        "        summary_metrics = eval_result.summary_metrics\n",
        "        mean_summary_metrics = [f\"{metric}/mean\" for metric in metrics]\n",
        "        updated_summary_metrics = []\n",
        "        if metrics:\n",
        "            for k, v in summary_metrics.items():\n",
        "                if k in mean_summary_metrics:\n",
        "                    updated_summary_metrics.append((k, v))\n",
        "            summary_metrics = dict(updated_summary_metrics)\n",
        "            # summary_metrics = {k: summary_metrics[k] for k, v in summary_metrics.items() if any(selected_metric in k for selected_metric in metrics)}\n",
        "\n",
        "        data.append(\n",
        "            go.Bar(\n",
        "                x=list(summary_metrics.keys()),\n",
        "                y=list(summary_metrics.values()),\n",
        "                name=title,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    fig = go.Figure(data=data)\n",
        "\n",
        "    # Change the bar mode\n",
        "    fig.update_layout(barmode=\"group\", showlegend=True)\n",
        "    fig.show()\n",
        "\n",
        "\n",
        "def generate_uuid(length: int = 8) -> str:\n",
        "    \"\"\"Generate a uuid of a specified length (default=8).\"\"\"\n",
        "    return \"\".join(random.choices(string.ascii_lowercase + string.digits, k=length))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXkKKzk8OvFq"
      },
      "source": [
        "## Load an evaluation dataset\n",
        "\n",
        "Load a subset of the `OpenOrca` dataset using the `huggingface/datasets` library. We will use 10 samples from the first 100 rows of the \"train\" split of `OpenOrca` dataset to demonstrate evaluating prompts and model responses in this Colab.\n",
        "\n",
        "### Dataset Summary\n",
        "\n",
        "The OpenOrca dataset is a collection of augmented [FLAN Collection data](https://arxiv.org/abs/2301.13688). Currently ~1M GPT-4 completions, and ~3.2M GPT-3.5 completions. It is tabularized in alignment with the distributions presented in the ORCA paper and currently represents a partial completion of the full intended dataset, with ongoing generation to expand its scope. The data is primarily used for training and evaluation in the field of natural language processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "w-j7l4Qd0Ull",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "c911d281868e4172abc9a93eb085ed35",
            "f8dc552d45584e5a8ecc03ec63056e46",
            "49047e0b27ce424fbee3df8dba050dc0",
            "826d125b401440d784c05e6c2a79c92e",
            "a89f7f5c0da8452eb45df77062b906ed",
            "618670e9467a44db8f54ff95b2afbf8f",
            "85315cd19bec4ce99167634a7efa1231",
            "a69db4b91988413eb44fd583b23ebc16",
            "0cf624db594e4736ab91658d423b8f89",
            "f008a807886647fbb4a7ff85ec73ce47",
            "3526e8d5841b40bfab3b82409711e302",
            "2fd8c6f51b214d4d8f33be4e9206decc",
            "05d5628b758842268116d4dbd2c79c28",
            "cfb22b50282e4000a70a74c07726e52f",
            "b7eea09116a340bfb7c28691ab86cf01",
            "8c160059ece9404083eb892c3778a00e",
            "58bcefef24ea4dd6958d6d165bcc0946",
            "21b4512319ba4300857b1f28e39b6de0",
            "ffb825002e674fc6810f6a2428eab854",
            "8f95f8dcca03426b8b1467aa67e1b898",
            "ecdb434942ac4a4bb124cae77b92141c",
            "10273425351d4adcb877d730b0c69672",
            "f7a35bfd3b2444e9bf491280c853f3b5",
            "d3e01540abef4fbba8e00cb472e38b02",
            "6e504c84457f45eb9d7b0537b83700d5",
            "a4907bb1017a4f4794bde1cab94cd435",
            "c137aba7af9c4d06aa6cff93217fe112",
            "39e972e00cd44ab4a1cf3f32f7918e4b",
            "5e55761a83bc48d7a4bc3cfdb1a7dafe",
            "d4d042cdfc1c4bfd98dc5c1365dd4031",
            "b589c830f232470c886cf85769980157",
            "df2a45413e804d9284a477a1751560dd",
            "1eae555773d8422189079b31000af21f"
          ]
        },
        "outputId": "8fabbe26-3bad-4f14-d0c7-763168e03e10"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/12.0k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c911d281868e4172abc9a93eb085ed35"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1M-GPT4-Augmented.parquet:   0%|          | 0.00/1.01G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2fd8c6f51b214d4d8f33be4e9206decc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7a35bfd3b2444e9bf491280c853f3b5"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = (\n",
        "    load_dataset(\n",
        "        \"Open-Orca/OpenOrca\",\n",
        "        data_files=\"1M-GPT4-Augmented.parquet\",\n",
        "        split=\"train[:100]\",\n",
        "    )\n",
        "    .to_pandas()\n",
        "    .drop(columns=[\"id\"])\n",
        "    .rename(columns={\"response\": \"reference\"})\n",
        ")\n",
        "\n",
        "dataset = ds.sample(n=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOeVlflg06F3"
      },
      "source": [
        "#### Preview the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "zAHGlmkEelhm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "9ec6fc43-5e13-40f1-f159-bd0346963116"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        system_prompt  \\\n",
              "71  You are an AI assistant. User will you give yo...   \n",
              "45  You are an AI assistant that helps people find...   \n",
              "95  You are an AI assistant. User will you give yo...   \n",
              "84  You are an AI assistant that follows instructi...   \n",
              "55  You are an AI assistant that follows instructi...   \n",
              "\n",
              "                                             question  \\\n",
              "71  Here is a goal: To hold down a keyboard note w...   \n",
              "45  Please answer the following question by reason...   \n",
              "95  Title: Next to useless if you actually want to...   \n",
              "84  Please answer the following question: Read the...   \n",
              "55  Please answer the following question: I am try...   \n",
              "\n",
              "                                            reference  \n",
              "71  To accomplish the goal of holding down a keybo...  \n",
              "45  - no.\\n\\nStep-by-step reasoning:\\n1. The state...  \n",
              "95  This review depicts the product in an unflatte...  \n",
              "84  Male specimens of the Perijá tapaculo have bro...  \n",
              "55  Spider-Man suspects that Dr. Connor is the Liz...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-01cef22a-da0b-4cd4-b729-f05fc1b1ed22\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>system_prompt</th>\n",
              "      <th>question</th>\n",
              "      <th>reference</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>You are an AI assistant. User will you give yo...</td>\n",
              "      <td>Here is a goal: To hold down a keyboard note w...</td>\n",
              "      <td>To accomplish the goal of holding down a keybo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>You are an AI assistant that helps people find...</td>\n",
              "      <td>Please answer the following question by reason...</td>\n",
              "      <td>- no.\\n\\nStep-by-step reasoning:\\n1. The state...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>You are an AI assistant. User will you give yo...</td>\n",
              "      <td>Title: Next to useless if you actually want to...</td>\n",
              "      <td>This review depicts the product in an unflatte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>You are an AI assistant that follows instructi...</td>\n",
              "      <td>Please answer the following question: Read the...</td>\n",
              "      <td>Male specimens of the Perijá tapaculo have bro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>You are an AI assistant that follows instructi...</td>\n",
              "      <td>Please answer the following question: I am try...</td>\n",
              "      <td>Spider-Man suspects that Dr. Connor is the Liz...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-01cef22a-da0b-4cd4-b729-f05fc1b1ed22')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-01cef22a-da0b-4cd4-b729-f05fc1b1ed22 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-01cef22a-da0b-4cd4-b729-f05fc1b1ed22');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4eb77d47-67b7-4bc7-b9c3-95c74421ef59\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4eb77d47-67b7-4bc7-b9c3-95c74421ef59')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4eb77d47-67b7-4bc7-b9c3-95c74421ef59 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dataset",
              "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"system_prompt\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"You are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.\",\n          \"You are an AI assistant that helps people find information.\",\n          \"Given a definition of a task and a sample input, break the definition into small parts.\\nEach of those parts will have some instruction. Explain their meaning by showing an example that meets the criteria in the instruction. Use the following format:\\nPart  # : a key part of the definition.\\nUsage: Sample response that meets the criteria from the key part. Explain why you think it meets the criteria.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Please answer the following question: Moeenuddin Ahmad Qureshi -  Moeenuddin Ahmad Qureshi usually referred to as Moeen Qureshi (born 1930) is a Pakistani economist and political figure. A former Vice President of the World Bank he was the Interim Prime Minister of Pakistan from July 18 1993 until 19 October 1993. Given a choice of categories company, educational institution, artist, athlete, office holder, mean of transportation, building, natural place, village, animal, plant, album, film or written work, the text refers to which one?\\nAnswer:\",\n          \"Please answer the following question by reasoning step-by-step. If \\\"A woman in a green shirt is welding.\\\" does that mean that \\\"A woman is cutting metal.\\\"?\\nOptions:\\n- yes\\n- it is not possible to tell\\n- no. Step-by-step reasoning:\",\n          \"I know that the answer to the question \\\"Who serves those who can perform good actions, producing merit?\\\" is in \\\"Theravadin Buddhists believe that personal effort is required to realize rebirth. Monks follow the vinaya: meditating, teaching and serving their lay communities. Laypersons can perform good actions, producing merit.\\\". Can you tell me what it is?\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"reference\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"The text refers to the category of \\\"office holder\\\" as Moeenuddin Ahmad Qureshi served as the Interim Prime Minister of Pakistan from July 18, 1993, until October 19, 1993.\",\n          \"- no.\\n\\nStep-by-step reasoning:\\n1. The statement given is \\\"A woman in a green shirt is welding.\\\"\\n2. Welding is a process where two or more metal pieces are joined together by heating their surfaces to the point of melting, usually with an electric arc or a gas flame.\\n3. Cutting metal, on the other hand, is a process of separating metal into two or more pieces using various methods, such as sawing, shearing, or plasma cutting.\\n4. Since welding and cutting metal are different processes, the statement \\\"A woman is cutting metal\\\" cannot be inferred from the given statement. Therefore, the answer is no.\",\n          \"The answer to the question \\\"Who serves those who can perform good actions, producing merit?\\\" is 'Monks.' They follow the vinaya, meditating, teaching, and serving their lay communities in Theravadin Buddhism.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgrS1Ljo0VVQ"
      },
      "source": [
        "## Run evaluation on 1P models\n",
        "\n",
        "The *Gen AI Evaluation Service SDK* includes native support for evaluating Gemini models. This streamlines the evaluation process for Google's latest and most capable family of large language models. With minimal coding effort, you can leverage pre-defined metrics and workflows to assess the performance of Gemini models on various tasks. You can also customize your own model-based metrics based on your specific evaluation criteria.\n",
        "\n",
        "This enhanced support enables you to:\n",
        "\n",
        "- **Quickly evaluate Gemini models:** Effortlessly assess the performance of Gemini models using the SDK's streamlined workflows.\n",
        "- **Compare models side-by-side:** Benchmark Gemini against other models to understand relative strengths and weaknesses.\n",
        "- **Analyze prompt templates:** Evaluate the effectiveness of different prompt designs for optimizing Gemini's performance.\n",
        "\n",
        "This native integration simplifies the evaluation process, allowing you to focus on understanding and improving the capabilities of Gemini."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "011966ad51c4"
      },
      "source": [
        "#### Understand the `EvalTask` class\n",
        "\n",
        "The `EvalTask` class is a core component of the *Gen AI Evaluation Service SDK* framework. It allows you to define and run evaluation jobs against your Gen AI models/applications, providing a structured way to measure their performance on specific tasks. Think of an `EvalTask` as a blueprint for your evaluation process.\n",
        "\n",
        "`EvalTask` class requires an evaluation dataset and a list of metrics. Supported metrics are documented on the Generative AI on Vertex AI [Define your evaluation metrics](https://cloud.google.com/vertex-ai/generative-ai/docs/models/determine-eval) page. The dataset can be an `pandas.DataFrame`, Python dictionary or a file path URI and can contain default column names such as `prompt`, `reference`, `response`, and `baseline_model_response`.\n",
        "\n",
        "* **Bring-your-own-response (BYOR):** You already have the data that you want to evaluate stored in the dataset. You can customize the response column names for both your model and the baseline model using parameters like `response_column_name` and `baseline_model_response_column_name` or through the `metric_column_mapping`.\n",
        "\n",
        "* **Perform model inference without a prompt template:** You have a dataset containing the input prompts to the model and want to perform model inference before evaluation. A column named `prompt` is required in the evaluation dataset and is used directly as input to the model.\n",
        "\n",
        "* **Perform model inference with a prompt template:** You have a dataset containing the input variables to the prompt template and want to assemble the prompts for model inference. Evaluation dataset must contain column names corresponding to the variable names in the prompt template. For example, if prompt template is \"Instruction: {instruction}, context: {context}\", the dataset must contain `instruction` and `context` columns.\n",
        "\n",
        "\n",
        "\n",
        "`EvalTask` supports extensive evaluation scenarios including BYOR, model inference with Gemini models, 3P models endpoints/SDK clients, or custom model generation functions, using computation-based metrics, model-based pointwise and pairwise metrics. The `evaluate()` method triggers the evaluation process, optionally taking a model, prompt template, experiment logging configuartions, and other evaluation run configurations. You can view the SDK reference documentation for [Gen AI Evaluation package](https://cloud.google.com/vertex-ai/generative-ai/docs/reference/python/latest/vertexai.evaluation) for more details."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noHGa6rD6-ks"
      },
      "source": [
        "### Define a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ol1fa0hI7Irr"
      },
      "outputs": [],
      "source": [
        "# Model to be evaluated\n",
        "model = GenerativeModel(\n",
        "    \"gemini-1.5-pro\",\n",
        "    generation_config={\"temperature\": 0.6, \"max_output_tokens\": 256, \"top_k\": 1},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xuvdzrUIayPp"
      },
      "source": [
        "### Use computation-based metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JrY-NLGg0TVS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 928
        },
        "outputId": "87e3a138-2e99-45ea-9d52-0076001c81e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:vertexai.evaluation._evaluation:Assembling prompts from the `prompt_template`. The `prompt` column in the `EvalResult.metrics_table` has the assembled prompts used for model response generation.\n",
            "INFO:vertexai.evaluation._evaluation:Generating a total of 10 responses from Gemini model gemini-1.5-pro.\n",
            "100%|██████████| 10/10 [00:00<00:00, 20.11it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PermissionDenied",
          "evalue": "403 Permission 'aiplatform.endpoints.predict' denied on resource '//aiplatform.googleapis.com/projects/qwiklabs-gcp-04-ab5c423b76d5/locations/us-central1/publishers/google/models/gemini-1.5-pro' (or it may not exist). [reason: \"IAM_PERMISSION_DENIED\"\ndomain: \"aiplatform.googleapis.com\"\nmetadata {\n  key: \"resource\"\n  value: \"projects/qwiklabs-gcp-04-ab5c423b76d5/locations/us-central1/publishers/google/models/gemini-1.5-pro\"\n}\nmetadata {\n  key: \"permission\"\n  value: \"aiplatform.endpoints.predict\"\n}\n]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/grpc/_interceptor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    276\u001b[0m     ) -> Any:\n\u001b[0;32m--> 277\u001b[0;31m         response, ignored_call = self._with_call(\n\u001b[0m\u001b[1;32m    278\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/grpc/_interceptor.py\u001b[0m in \u001b[0;36m_with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    331\u001b[0m         )\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;34m\"\"\"See grpc.Future.result.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/grpc/_interceptor.py\u001b[0m in \u001b[0;36mcontinuation\u001b[0;34m(new_details, request)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                 response, call = self._thunk(new_method).with_call(\n\u001b[0m\u001b[1;32m    316\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36mwith_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1197\u001b[0m         )\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pytype: disable=not-instantiable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.PERMISSION_DENIED\n\tdetails = \"Permission 'aiplatform.endpoints.predict' denied on resource '//aiplatform.googleapis.com/projects/qwiklabs-gcp-04-ab5c423b76d5/locations/us-central1/publishers/google/models/gemini-1.5-pro' (or it may not exist).\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:74.125.196.95:443 {created_time:\"2025-02-13T03:52:45.392006264+00:00\", grpc_status:7, grpc_message:\"Permission \\'aiplatform.endpoints.predict\\' denied on resource \\'//aiplatform.googleapis.com/projects/qwiklabs-gcp-04-ab5c423b76d5/locations/us-central1/publishers/google/models/gemini-1.5-pro\\' (or it may not exist).\"}\"\n>",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mPermissionDenied\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-f845d68814c0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"rouge_l_sum\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n\u001b[0;32m----> 6\u001b[0;31m rouge_result = rouge_eval_task.evaluate(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprompt_template\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"# System_prompt\\n{system_prompt} # Question\\n{question}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vertexai/evaluation/eval_task.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, model, prompt_template, experiment_run_name, response_column_name, baseline_model_response_column_name, evaluation_service_qps, retry_timeout, output_file_name)\u001b[0m\n\u001b[1;32m    461\u001b[0m             )\n\u001b[1;32m    462\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m             eval_result = _evaluation.evaluate(\n\u001b[0m\u001b[1;32m    464\u001b[0m                 \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vertexai/evaluation/_evaluation.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(dataset, metrics, model, prompt_template, metric_column_mapping, evaluation_service_qps, retry_timeout)\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[0m_assemble_prompt_for_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation_run_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_template\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m     _run_model_inference(\n\u001b[0m\u001b[1;32m    937\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0mevaluation_run_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluation_run_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vertexai/evaluation/_evaluation.py\u001b[0m in \u001b[0;36m_run_model_inference\u001b[0;34m(model, evaluation_run_config, response_column_name)\u001b[0m\n\u001b[1;32m    467\u001b[0m                 \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerative_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerativeModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m                     _generate_responses_from_gemini_model(\n\u001b[0m\u001b[1;32m    470\u001b[0m                         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluation_run_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_baseline_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vertexai/evaluation/_evaluation.py\u001b[0m in \u001b[0;36m_generate_responses_from_gemini_model\u001b[0;34m(model, evaluation_run_config, is_baseline_model)\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_done_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mresponses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_baseline_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mevaluation_run_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_model_response\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vertexai/evaluation/_evaluation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    382\u001b[0m                 \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_done_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mresponses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_baseline_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0mevaluation_run_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline_model_response\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresponses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vertexai/evaluation/_evaluation.py\u001b[0m in \u001b[0;36m_generate_content_text_response\u001b[0;34m(model, prompt)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0msafety\u001b[0m \u001b[0mreasons\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \"\"\"\n\u001b[0;32m--> 321\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vertexai/generative_models/_generative_models.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, contents, generation_config, safety_settings, tools, tool_config, labels, stream)\u001b[0m\n\u001b[1;32m    693\u001b[0m             )\n\u001b[1;32m    694\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m             return self._generate_content(\n\u001b[0m\u001b[1;32m    696\u001b[0m                 \u001b[0mcontents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m                 \u001b[0mgeneration_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgeneration_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/vertexai/generative_models/_generative_models.py\u001b[0m in \u001b[0;36m_generate_content\u001b[0;34m(self, contents, generation_config, safety_settings, tools, tool_config, labels)\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m         )\n\u001b[0;32m--> 820\u001b[0;31m         \u001b[0mgapic_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prediction_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgapic_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py\u001b[0m in \u001b[0;36mgenerate_content\u001b[0;34m(self, request, model, contents, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   2228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2229\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2230\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m   2231\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2232\u001b[0m             \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compression\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0merror_remapped_callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPermissionDenied\u001b[0m: 403 Permission 'aiplatform.endpoints.predict' denied on resource '//aiplatform.googleapis.com/projects/qwiklabs-gcp-04-ab5c423b76d5/locations/us-central1/publishers/google/models/gemini-1.5-pro' (or it may not exist). [reason: \"IAM_PERMISSION_DENIED\"\ndomain: \"aiplatform.googleapis.com\"\nmetadata {\n  key: \"resource\"\n  value: \"projects/qwiklabs-gcp-04-ab5c423b76d5/locations/us-central1/publishers/google/models/gemini-1.5-pro\"\n}\nmetadata {\n  key: \"permission\"\n  value: \"aiplatform.endpoints.predict\"\n}\n]"
          ]
        }
      ],
      "source": [
        "# Define an EvalTask with ROUGE-L-SUM metric\n",
        "rouge_eval_task = EvalTask(\n",
        "    dataset=dataset,\n",
        "    metrics=[\"rouge_l_sum\"],\n",
        ")\n",
        "rouge_result = rouge_eval_task.evaluate(\n",
        "    model=model,\n",
        "    prompt_template=\"# System_prompt\\n{system_prompt} # Question\\n{question}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "FSMRnDMixwLS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "3ab97bc2-9d1e-43a1-9b33-f31c66abe065"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'display_eval_result' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-9fd8899c28ed>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdisplay_eval_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrouge_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'display_eval_result' is not defined"
          ]
        }
      ],
      "source": [
        "display_eval_result(rouge_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx9pKjiSbHag"
      },
      "source": [
        "### Use model-based pointwise metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCmPRv4kHwPG"
      },
      "source": [
        "#### Select a pointwise metric to use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b88aa7d862a"
      },
      "outputs": [],
      "source": [
        "supported_example_metric_names = (\n",
        "    MetricPromptTemplateExamples.list_example_metric_names()\n",
        ")\n",
        "\n",
        "for metric in supported_example_metric_names:\n",
        "    print(metric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuRlYbj0HvBB"
      },
      "outputs": [],
      "source": [
        "import ipywidgets as widgets\n",
        "\n",
        "pointwise_single_turn_metrics = [\n",
        "    metric\n",
        "    for metric in supported_example_metric_names\n",
        "    if not metric.startswith(\"pairwise\") and not metric.startswith(\"multi_turn\")\n",
        "]\n",
        "\n",
        "dropdown = widgets.Dropdown(\n",
        "    options=pointwise_single_turn_metrics,\n",
        "    description=\"Select a metric:\",\n",
        "    font_weight=\"bold\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        ")\n",
        "\n",
        "\n",
        "def dropdown_eventhandler(change):\n",
        "    global POINTWISE_METRIC\n",
        "    if change[\"type\"] == \"change\" and change[\"name\"] == \"value\":\n",
        "        POINTWISE_METRIC = change.new\n",
        "        print(\"Selected:\", change.new)\n",
        "\n",
        "\n",
        "POINTWISE_METRIC = dropdown.value\n",
        "dropdown.observe(dropdown_eventhandler, names=\"value\")\n",
        "display(dropdown)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBnyzogabOmp"
      },
      "outputs": [],
      "source": [
        "# Define an EvalTask with a pointwise model-based metric\n",
        "pointwise_eval_task = EvalTask(\n",
        "    dataset=dataset,\n",
        "    metrics=[POINTWISE_METRIC],\n",
        ")\n",
        "\n",
        "pointwise_result = pointwise_eval_task.evaluate(\n",
        "    model=model,\n",
        "    prompt_template=\"# System_prompt\\n{system_prompt} # Question\\n{question}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EwLaPusyeL9"
      },
      "outputs": [],
      "source": [
        "display_eval_result(pointwise_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qsPD-YUWoRYo"
      },
      "outputs": [],
      "source": [
        "display_explanations(pointwise_result, metrics=[POINTWISE_METRIC], n=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AzW8Zg71Igs"
      },
      "source": [
        "#### Build your own pointwise metric\n",
        "\n",
        "For more inforamation about metric customization, see this [notebook tutorial](https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/evaluation/customize_model_based_metrics.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZKVjolkE5qoh"
      },
      "outputs": [],
      "source": [
        "# Create a unique model-based metric for your own use cases\n",
        "linguistic_acceptability = PointwiseMetric(\n",
        "    metric=\"linguistic_acceptability\",\n",
        "    metric_prompt_template=PointwiseMetricPromptTemplate(\n",
        "        criteria={\n",
        "            \"Proper Grammar\": \"The language's grammar rules are correctly followed, including but not limited to sentence structures, verb tenses, subject-verb agreement, proper punctuation, and capitalization.\",\n",
        "            \"Appropriate word choice\": \"Words chosen are appropriate and purposeful given their relative context and positioning in the text. Vocabulary demonstrates prompt understanding.\",\n",
        "            \"Reference Alignment\": \"The response is consistent and aligned with the reference.\",\n",
        "        },\n",
        "        rating_rubric={\n",
        "            \"5\": \"Excellent: The writing is grammatically correct, uses appropriate vocabulary and aligns perfectly with the reference.\",\n",
        "            \"4\": \"Good: The writing is generally grammatically correct, uses appropriate vocabulary and aligns well with the reference.\",\n",
        "            \"3\": \"Satisfactory: The writing may have minor grammatical errors or use less-appropriate vocabulary, but it aligns reasonably well with the reference.\",\n",
        "            \"2\": \"Unsatisfactory: The writing has significant grammatical errors, uses inappropriate vocabulary, deviates significantly from the reference.\",\n",
        "            \"1\": \"Poor: The writing is riddled with grammatical errors, uses highly inappropriate vocabulary, is completely unrelated to the reference.\",\n",
        "        },\n",
        "        input_variables=[\"prompt\", \"reference\"],\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MJdLUJz6H5F"
      },
      "outputs": [],
      "source": [
        "pointwise_eval_task = EvalTask(\n",
        "    dataset=dataset,\n",
        "    metrics=[linguistic_acceptability],\n",
        ")\n",
        "\n",
        "pointwise_result = pointwise_eval_task.evaluate(\n",
        "    model=model,\n",
        "    prompt_template=\"# System_prompt\\n{system_prompt} # Question\\n{question}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UW5DXIGv7oIJ"
      },
      "outputs": [],
      "source": [
        "display_eval_result(pointwise_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFHRuWjv86uG"
      },
      "source": [
        "### Use model-based pairwise metrics\n",
        "\n",
        "Evaluate two Gen AI models side-by-side (SxS) with model-based pairwise metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Rd0qIOrMBRb"
      },
      "source": [
        "#### Select a pairwise metric to use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E7Eqh9e9MDsa"
      },
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "pairwise_single_turn_metrics = [\n",
        "    metric\n",
        "    for metric in supported_example_metric_names\n",
        "    if metric.startswith(\"pairwise\") and \"multi_turn\" not in metric\n",
        "]\n",
        "\n",
        "dropdown = widgets.Dropdown(\n",
        "    options=pairwise_single_turn_metrics,\n",
        "    description=\"Select a metric:\",\n",
        "    font_weight=\"bold\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        ")\n",
        "\n",
        "\n",
        "def dropdown_eventhandler(change):\n",
        "    global pairwise_metric_name\n",
        "    if change[\"type\"] == \"change\" and change[\"name\"] == \"value\":\n",
        "        pairwise_metric = change.new\n",
        "        print(\"Selected:\", change.new)\n",
        "\n",
        "\n",
        "pairwise_metric_name = dropdown.value\n",
        "dropdown.observe(dropdown_eventhandler, names=\"value\")\n",
        "display(dropdown)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLTaArLre0TE"
      },
      "source": [
        "#### Define a baseline model to compare against"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4u-WJVK2-SI5"
      },
      "outputs": [],
      "source": [
        "# Define a baseline model for pairwise comparison\n",
        "baseline_model = GenerativeModel(\"gemini-1.5-flash-001\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJd6sF7re5gC"
      },
      "source": [
        "#### Run SxS evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yoZrZsiM4Sn"
      },
      "outputs": [],
      "source": [
        "# Create a pairwise metric\n",
        "PAIRWISE_METRIC = PairwiseMetric(\n",
        "    metric=pairwise_metric_name,\n",
        "    metric_prompt_template=MetricPromptTemplateExamples.get_prompt_template(\n",
        "        pairwise_metric_name\n",
        "    ),\n",
        "    baseline_model=baseline_model,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDAaYDqY9FBu"
      },
      "outputs": [],
      "source": [
        "pairwise_eval_task = EvalTask(\n",
        "    dataset=dataset,\n",
        "    metrics=[PAIRWISE_METRIC],\n",
        ")\n",
        "# Specify a candidate model for pairwise comparison\n",
        "pairwise_result = pairwise_eval_task.evaluate(\n",
        "    model=model,\n",
        "    prompt_template=\"# System_prompt\\n{system_prompt} # Question\\n{question}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPAoJu53-mfx"
      },
      "outputs": [],
      "source": [
        "display_eval_result(\n",
        "    pairwise_result,\n",
        "    title=\"Gemini-1.5-Flash vs. Gemini-1.5-Pro SxS Pairwise Evaluation Results\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olbyG8ZVOvFq"
      },
      "source": [
        "## Run evaluation on 3P Models\n",
        "\n",
        "The *Vertex Gen AI Evaluation Service SDK* provides robust support for evaluating third-party (3P) models, allowing you to assess and compare models from various sources. The *Gen AI Evaluation Service SDK* allows you to provide a generic Python function as input to specify how the model/application should be invoked for batch inference, which could be done through an endpoint or an SDK. This flexible approach accommodates a wide range of open and closed models.\n",
        "\n",
        "**Open Models:**\n",
        "\n",
        "Evaluate open models like a pre-trained `Llama 3.1` or a fine-tuned `Llama 3` models deployed with Vertex AI Model Garden using the *Gen AI Evaluation Service SDK*. This enables you to assess the performance of these models and understand how they align with your specific requirements.\n",
        "\n",
        "**Closed Models:**\n",
        "\n",
        "Evaluate the performance of closed model APIs, such as Anthropic's `Claude 3.5 Sonnet`, hosted on Vertex AI. This allows you to compare the capabilities of different closed models and make informed decisions about which best suits your needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXfM_yJwiuh3"
      },
      "outputs": [],
      "source": [
        "# Define an EvalTask with a list of metrics\n",
        "pointwise_eval_task = EvalTask(\n",
        "    dataset=dataset,\n",
        "    metrics=[\n",
        "        \"coherence\",\n",
        "        \"fluency\",\n",
        "        \"instruction_following\",\n",
        "        \"text_quality\",\n",
        "        \"rouge_l_sum\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGokrtdiIHrX"
      },
      "source": [
        "### Run Evaluation on Llama 3.1 API Endpoint\n",
        "\n",
        "You can experiment with various supported Llama models.\n",
        "\n",
        "This tutorial use Llama 3 8B Instruct, 70B Instruct, and 405B Instruct using Model-as-a-Service (MaaS). Using Model-as-a-Service (MaaS), you can access Llama 3.1 models in just a few clicks without any setup or infrastructure hassles. Model-as-a-Service (MaaS) integrates [Llama Guard](https://huggingface.co/meta-llama/Llama-Guard-3-8B) as a safety filter. It is switched on by default and can be switched off. Llama Guard enables us to safeguard model inputs and outputs. If a response is filtered, it will be populated with a `finish_reason` field (with value `content_filtered`) and a `refusal` field (stating the filtering reason).\n",
        "\n",
        "You can also access Llama models for self-service in Vertex AI Model Garden, allowing you to choose your preferred infrastructure. [Check out Llama 3.1 model card](https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama3_1?_ga=2.31261500.2048242469.1721714335-1107467625.1721655511) to learn how to deploy a Llama 3.1 models on Vertex AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7OhyH46H2H5"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = \"meta/llama3-8b-instruct-maas\"  # @param {type:\"string\"} [\"meta/llama3-8b-instruct-maas\", \"meta/llama3-70b-instruct-maas\", \"meta/llama3-405b-instruct-maas\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUgMmggOa_TG"
      },
      "source": [
        "#### Authentication\n",
        "\n",
        "You can request an access token from the default credentials for the current environment. Note that the access token lives for [1 hour by default](https://cloud.google.com/docs/authentication/token-types#at-lifetime); after expiration, it must be refreshed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OT03q0Oqa86s"
      },
      "outputs": [],
      "source": [
        "credentials, _ = default()\n",
        "auth_request = transport.requests.Request()\n",
        "credentials.refresh(auth_request)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtpB2ig9bCxs"
      },
      "source": [
        "Then configure the OpenAI SDK to point to the Llama 3.1 API endpoint.\n",
        "\n",
        "Note: only `us-central1` is supported region for Llama 3.1 models using Model-as-a-Service (MaaS)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TazdE-CbEdW"
      },
      "outputs": [],
      "source": [
        "MODEL_LOCATION = \"us-central1\"\n",
        "\n",
        "client = openai.OpenAI(\n",
        "    base_url=f\"https://{MODEL_LOCATION}-aiplatform.googleapis.com/v1beta1/projects/{PROJECT_ID}/locations/{MODEL_LOCATION}/endpoints/openapi/chat/completions?\",\n",
        "    api_key=credentials.token,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMPNCDTsbqwc"
      },
      "source": [
        "#### Set model configurations for Llama 3.1\n",
        "\n",
        "Use the following parameters to generate different answers:\n",
        "\n",
        "*   `temperature` to control the randomness of the response\n",
        "*   `max_tokens` to limit the response length\n",
        "*   `top_p` to control the quality of the response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VxQbGCHHbnot"
      },
      "outputs": [],
      "source": [
        "temperature = 1.0  # @param {type:\"number\"}\n",
        "max_tokens = 50  # @param {type:\"integer\"}\n",
        "top_p = 1.0  # @param {type:\"number\"}\n",
        "apply_llama_guard = True  # @param {type:\"boolean\"}\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=MODEL_ID,\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Hello, Llama 3.1!\"}],\n",
        "    extra_body={\n",
        "        \"extra_body\": {\n",
        "            \"google\": {\n",
        "                \"model_safety_settings\": {\n",
        "                    \"enabled\": apply_llama_guard,\n",
        "                    \"llama_guard_settings\": {},\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    },\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqoSsvMlcYPM"
      },
      "source": [
        "#### Define the Llama 3.1 Model Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdiP_zMWbzp0"
      },
      "outputs": [],
      "source": [
        "def llama_model_fn(prompt: str) -> str:\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL_ID,\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "        top_p=top_p,\n",
        "        extra_body={\n",
        "            \"extra_body\": {\n",
        "                \"google\": {\n",
        "                    \"model_safety_settings\": {\n",
        "                        \"enabled\": apply_llama_guard,\n",
        "                        \"llama_guard_settings\": {},\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmtUTC1jTZT2"
      },
      "source": [
        "#### Run evaluation on Llama 3.1 API Service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1are_m2DTZT2"
      },
      "outputs": [],
      "source": [
        "llama_result = pointwise_eval_task.evaluate(\n",
        "    model=llama_model_fn,\n",
        "    prompt_template=\"# System_prompt\\n{system_prompt} # Question\\n{question}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gD3zu0LLTZT2"
      },
      "outputs": [],
      "source": [
        "display_eval_result(llama_result, title=\"Llama 3.1 API Service Evaluation Results\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUWSXVToTZT2"
      },
      "outputs": [],
      "source": [
        "display_explanations(llama_result, n=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS2BpbzMIpZc"
      },
      "source": [
        "### Run Evaluation on Claude 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4j9taDaPJHy9"
      },
      "outputs": [],
      "source": [
        "MODEL = \"claude-3-5-sonnet@20240620\"  # @param [\"claude-3-5-sonnet@20240620\", \"claude-3-opus@20240229\", \"claude-3-haiku@20240307\", \"claude-3-sonnet@20240229\" ]\n",
        "if MODEL == \"claude-3-5-sonnet@20240620\":\n",
        "    available_regions = [\"europe-west1\", \"us-east5\"]\n",
        "elif MODEL == \"claude-3-opus@20240229\":\n",
        "    available_regions = [\"us-east5\"]\n",
        "elif MODEL == \"claude-3-haiku@20240307\":\n",
        "    available_regions = [\"us-east5\", \"europe-west1\"]\n",
        "elif MODEL == \"claude-3-sonnet@20240229\":\n",
        "    available_regions = [\"us-east5\"]\n",
        "\n",
        "dropdown = widgets.Dropdown(\n",
        "    options=available_regions,\n",
        "    description=\"Select a location:\",\n",
        "    font_weight=\"bold\",\n",
        "    style={\"description_width\": \"initial\"},\n",
        ")\n",
        "\n",
        "\n",
        "def dropdown_eventhandler(change):\n",
        "    global LOCATION\n",
        "    if change[\"type\"] == \"change\" and change[\"name\"] == \"value\":\n",
        "        LOCATION = change.new\n",
        "        print(\"Selected:\", change.new)\n",
        "\n",
        "\n",
        "LOCATION = dropdown.value\n",
        "dropdown.observe(dropdown_eventhandler, names=\"value\")\n",
        "display(dropdown)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVbFk9pSd6L0"
      },
      "source": [
        "#### Define the Claude 3 Model Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKLjm0o3KwEI"
      },
      "outputs": [],
      "source": [
        "def anthropic_claude_model_fn(prompt):\n",
        "    client = AnthropicVertex(region=LOCATION, project_id=PROJECT_ID)\n",
        "    message = client.messages.create(\n",
        "        max_tokens=1024,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"{prompt}\",\n",
        "            }\n",
        "        ],\n",
        "        model=MODEL,\n",
        "    )\n",
        "    response = message.content[0].text\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N73OcARGgMUT"
      },
      "source": [
        "#### Run evaluation on Claude 3 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdGUsc7gIpZi"
      },
      "outputs": [],
      "source": [
        "claude_result = pointwise_eval_task.evaluate(\n",
        "    model=anthropic_claude_model_fn,\n",
        "    prompt_template=\"# System_prompt\\n{system_prompt} # Question\\n{question}\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_URkaZhYIpZi"
      },
      "outputs": [],
      "source": [
        "display_eval_result(claude_result, title=\"Claude-3.5-Sonnet Evaluation Results\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38IJUEXthXsE"
      },
      "outputs": [],
      "source": [
        "display_explanations(claude_result, n=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLHuqe2D-x81"
      },
      "source": [
        "## Prompt Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd393d80e252"
      },
      "source": [
        "The *Vertex AI Gen AI Evaluation Service SDK* simplifies prompt engineering by streamlining the process of creating and evaluating multiple prompt templates. It allows you to efficiently test different prompts against a chosen dataset and compare their performance using comprehensive evaluation metrics. This empowers you to identify the most effective prompts for your specific use case and optimize your generative AI applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "601693fa0461"
      },
      "source": [
        "### Design a prompt with Prompt Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a53e4c50779f"
      },
      "outputs": [],
      "source": [
        "system_instruction = \"You are a poetic assistant, skilled in explaining complex concepts with creative flair.\"\n",
        "question = \"How does LLM work?\"\n",
        "requirements = \"Explain concepts in great depth using simple terms, and give examples to help people learn. At the end of each explanation, you ask a question to check for understanding\"\n",
        "\n",
        "prompt_template = f\"{system_instruction} Answer this question: {question}, and follow the requirements: {requirements}.\"\n",
        "\n",
        "\n",
        "model_response = (\n",
        "    GenerativeModel(\"gemini-1.5-pro\")\n",
        "    .generate_content(prompt_template)\n",
        "    .candidates[0]\n",
        "    .content.parts[0]\n",
        "    .text\n",
        ")\n",
        "\n",
        "\n",
        "display(HTML(f\"<h2>Assembled Prompt:</h2><hr><h3>{prompt_template}</h3>\"))\n",
        "display(HTML(\"<h2>Model Response: </h2><hr>\"))\n",
        "Markdown(model_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5261f69b615a"
      },
      "source": [
        "###  Compare and optimize prompt template design"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d01f70c3163"
      },
      "source": [
        "#### Define an evaluation dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2157353f3309"
      },
      "source": [
        "To perform pointwise inference, the evaluation dataset is required to contain the following fields:\n",
        "\n",
        "* Instruction: Part of the input user prompt. It refers to the inference instruction that is sent to your LLM.\n",
        "* Context: User input for the Gen AI model or application in the current turn.\n",
        "* Reference: The ground truth to compare your LLM response to.\n",
        "\n",
        "Your dataset must include a minimum of one evaluation example. We recommend around 100 examples to ensure high-quality aggregated metrics and statistically significant results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "972ebabb2712"
      },
      "outputs": [],
      "source": [
        "instruction = \"Summarize the following article: \\n\"\n",
        "\n",
        "context = [\n",
        "    \"Typhoon Phanfone has killed at least one person, a US airman on Okinawa who was washed away by high waves. Thousands of households have lost power and Japan's two largest airlines have suspended many flights. The storm also forced the suspension of the search for people missing after last week's volcanic eruption. The storm-tracking website Tropical Storm Risk forecasts that Phanfone will rapidly lose power over the next few hours as it goes further into the Pacific Ocean. Typhoon Phanfone was downgraded from an earlier status of a super typhoon, but the Japan Meteorological Agency had warned it was still a dangerous storm. Japan averages 11 typhoons a year, according to its weather agency. The typhoon made landfall on Monday morning near the central city of Hamamatsu, with winds of up to 180 km/h (112 mph). The airman was one of three US military personnel swept away by high waves whipped up by the typhoon off southern Okinawa island, where the US has a large military base. The remaining two are still missing. A police spokesman said they had been taking photographs of the sea. A university student who was surfing off the seas of Kanagawa Prefecture, south of Tokyo, was also missing, national broadcast NHK reports. It said at least 10 people had been injured and 9,500 houses were without power. The storm was expected to deposit about 100mm of rain on Tokyo over 24 hours, according to the Transport Ministry website. Many schools were closed on Monday and two car companies in Japan halted production at some plants ahead of the storm. More than 174 domestic flights were affected nationwide, NHK state broadcaster said on Sunday. On Sunday, heavy rain delayed the Japanese Formula One Grand Prix in Suzaka. French driver Jules Bianchi lost control in the wet conditions and crashed, sustaining a severe head injury.\",\n",
        "    \"The blaze started at the detached building in Drivers End in Codicote, near Welwyn, during the morning. There was another fire at the building 20 years ago, after which fire-proof foil was placed under the thatch, which is protecting the main building. More than 15 fire engines and support vehicles were called to tackle the blaze. Roads in the area were closed and traffic diverted.\",\n",
        "    'The 18-year-old fell at the New Charter Academy on Broadoak Road in Ashton-under-Lyne at about 09:10 BST, Greater Manchester Police (GMP) said. GMP said he had gone to Manchester Royal Infirmary and his condition was \"serious\". Principal Jenny Langley said the school would remain \"fully open\" while police investigated. \"Our thoughts are with the family and we\\'re doing everything we can to support them along with staff and pupils,\" she said.',\n",
        "    'But Belgian-born Dutchman Max Verstappen was unable to drive a car legally on his own in either country. That all changed on Wednesday when the youngster turned 18 and passed his driving test at the first attempt. Despite having competed in 14 grands prix since his debut in Australia in March, Verstappen admitted to feeling the pressure during his test. \"It\\'s a relief,\" said the Toro Rosso driver, who finished ninth in Japan on Sunday and had only started driving lessons a week ago. \"I was a bit nervous to make mistakes, but the exam went well.\" A bonus of turning 18 is that Verstappen will now be able to drink the champagne if he ever makes it onto the podium.',\n",
        "]\n",
        "\n",
        "reference = [\n",
        "    \"A powerful typhoon has brought many parts of Japan to a standstill and briefly battered Tokyo before heading out to sea.\",\n",
        "    \"A major fire has been burning in the thatched roof of a large property in Hertfordshire.\",\n",
        "    \"A student has been taken to hospital after falling from a balcony at a Greater Manchester school.\",\n",
        "    \"He is Formula 1's youngest ever driver and in charge of a car that can reach over 200mph.\",\n",
        "]\n",
        "\n",
        "response = [\n",
        "    \"Typhoon Phanfone, while downgraded from super typhoon status, caused significant disruption and tragedy in Japan. One US airman died after being swept away by high waves, with two more missing. The storm caused power outages for thousands, flight cancellations, and the suspension of rescue efforts for missing volcano victims. Heavy rain and strong winds led to school and factory closures, transportation disruptions, and at least 10 injuries. The typhoon is expected to weaken as it moves over the Pacific Ocean.\",\n",
        "    \"A large fire broke out in a detached thatched building in Codicote, near Welwyn. This is the second fire at the building in 20 years. Thankfully, fire-proof foil installed after the previous fire is protecting the main building. Over 15 fire engines and support vehicles responded, closing roads and diverting traffic in the area.\",\n",
        "    \"An 18-year-old student at New Charter Academy in Ashton-under-Lyne suffered a serious fall and was hospitalized. The incident is under investigation by Greater Manchester Police, but the school remains open. The principal expressed support for the student's family and the school community.\",\n",
        "    \"Max Verstappen, a Formula One driver, was finally able to get his driver's license at age 18. Despite already competing in 14 Grand Prix races, he was not of legal driving age in his native countries. He admitted to being nervous but passed the test on his first attempt.  As an added bonus of turning 18, Verstappen can now enjoy champagne on the podium if he places.\",\n",
        "]\n",
        "\n",
        "eval_dataset = pd.DataFrame(\n",
        "    {\n",
        "        \"instruction\": instruction,\n",
        "        \"context\": context,\n",
        "        \"reference\": reference,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baff9d1cca96"
      },
      "source": [
        "#### Define prompt templates to compare\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f639a79316c1"
      },
      "outputs": [],
      "source": [
        "prompt_templates = [\n",
        "    \"Instruction: {instruction} such that you're explaining it to a 5 year old. Article: {context}. Summary:\",\n",
        "    \"Article: {context}. Complete this task: {instruction}. Summary:\",\n",
        "    \"Goal: {instruction} and give me a TL;DR in five words. Here's an article: {context}. Summary:\",\n",
        "    \"Article: {context}. Reference Summary: {reference}. {instruction} to be more concise and verbose than the reference.\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncJ-4uA_nxNB"
      },
      "source": [
        "#### Define a model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3155f46d31c6"
      },
      "outputs": [],
      "source": [
        "generation_config = {\"temperature\": 0.3, \"max_output_tokens\": 256, \"top_k\": 1}\n",
        "\n",
        "gemini_model = GenerativeModel(\n",
        "    \"gemini-1.5-pro\",\n",
        "    generation_config=generation_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3c1121684f5"
      },
      "source": [
        "#### Define an EvalTask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20eb95c14422"
      },
      "outputs": [],
      "source": [
        "metrics = [\n",
        "    \"rouge_l_sum\",\n",
        "    \"bleu\",\n",
        "    \"fluency\",\n",
        "    \"coherence\",\n",
        "    \"safety\",\n",
        "    \"groundedness\",\n",
        "    \"summarization_quality\",\n",
        "    \"verbosity\",\n",
        "    \"instruction_following\",\n",
        "    \"text_quality\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "052ec86e5777"
      },
      "outputs": [],
      "source": [
        "EXPERIMENT_NAME = \"eval-sdk-prompt-engineering\"  # @param {type:\"string\"}\n",
        "\n",
        "summarization_eval_task = EvalTask(\n",
        "    dataset=eval_dataset,\n",
        "    metrics=metrics,\n",
        "    experiment=EXPERIMENT_NAME,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "515fe8c3652f"
      },
      "source": [
        "#### Run Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b8422213915"
      },
      "outputs": [],
      "source": [
        "eval_results = []\n",
        "for i, prompt_template in enumerate(prompt_templates):\n",
        "    eval_result = summarization_eval_task.evaluate(\n",
        "        prompt_template=prompt_template,\n",
        "        model=gemini_model,\n",
        "        # Customize eval service rate limit based on your project's Gemini-1.5-pro model quota to improve speed.\n",
        "        # See more details in https://cloud.google.com/vertex-ai/generative-ai/docs/models/run-evaluation#increase-quota\n",
        "        evaluation_service_qps=1,\n",
        "    )\n",
        "\n",
        "    eval_results.append((f\"Prompt Template #{i+1}\", eval_result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec6e97cf9f27"
      },
      "source": [
        "#### Display Evaluation report and explanations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3724b416b39"
      },
      "outputs": [],
      "source": [
        "for result in eval_results:\n",
        "    display_eval_result(title=result[0], eval_result=result[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0615d0925469"
      },
      "outputs": [],
      "source": [
        "for eval_result in eval_results:\n",
        "    display_explanations(eval_result[1], metrics=[\"summarization_quality\"], n=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzWSUPj2oV-_"
      },
      "source": [
        "#### Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSDyxE2aoYMH"
      },
      "outputs": [],
      "source": [
        "display_radar_plot(\n",
        "    eval_results,\n",
        "    metrics=[\"instruction_following\", \"fluency\", \"coherence\", \"text_quality\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Id5sjYHboZHh"
      },
      "outputs": [],
      "source": [
        "display_bar_plot(\n",
        "    eval_results,\n",
        "    metrics=[\"instruction_following\", \"fluency\", \"coherence\", \"text_quality\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed966819648e"
      },
      "source": [
        "####  View Experiment log for evaluation runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "363c1b2553b9"
      },
      "outputs": [],
      "source": [
        "summarization_eval_task.display_runs()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7cce28cc97e"
      },
      "source": [
        "## Cleaning up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef73672573e1"
      },
      "outputs": [],
      "source": [
        "delete_experiment = True\n",
        "\n",
        "# Please set your LOCATION to the same one used during Vertex AI SDK initialization.\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "if delete_experiment:\n",
        "\n",
        "    from google.cloud import aiplatform\n",
        "\n",
        "    aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
        "    experiment = aiplatform.Experiment(EXPERIMENT_NAME)\n",
        "    experiment.delete()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "tfQ7sPtOjZOw",
        "F_Gw6YLeOvFq"
      ],
      "name": "evaluate_models_in_vertex_ai_studio_and_model_garden.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c911d281868e4172abc9a93eb085ed35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f8dc552d45584e5a8ecc03ec63056e46",
              "IPY_MODEL_49047e0b27ce424fbee3df8dba050dc0",
              "IPY_MODEL_826d125b401440d784c05e6c2a79c92e"
            ],
            "layout": "IPY_MODEL_a89f7f5c0da8452eb45df77062b906ed"
          }
        },
        "f8dc552d45584e5a8ecc03ec63056e46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_618670e9467a44db8f54ff95b2afbf8f",
            "placeholder": "​",
            "style": "IPY_MODEL_85315cd19bec4ce99167634a7efa1231",
            "value": "README.md: 100%"
          }
        },
        "49047e0b27ce424fbee3df8dba050dc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a69db4b91988413eb44fd583b23ebc16",
            "max": 11966,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0cf624db594e4736ab91658d423b8f89",
            "value": 11966
          }
        },
        "826d125b401440d784c05e6c2a79c92e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f008a807886647fbb4a7ff85ec73ce47",
            "placeholder": "​",
            "style": "IPY_MODEL_3526e8d5841b40bfab3b82409711e302",
            "value": " 12.0k/12.0k [00:00&lt;00:00, 794kB/s]"
          }
        },
        "a89f7f5c0da8452eb45df77062b906ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "618670e9467a44db8f54ff95b2afbf8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85315cd19bec4ce99167634a7efa1231": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a69db4b91988413eb44fd583b23ebc16": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cf624db594e4736ab91658d423b8f89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f008a807886647fbb4a7ff85ec73ce47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3526e8d5841b40bfab3b82409711e302": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2fd8c6f51b214d4d8f33be4e9206decc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_05d5628b758842268116d4dbd2c79c28",
              "IPY_MODEL_cfb22b50282e4000a70a74c07726e52f",
              "IPY_MODEL_b7eea09116a340bfb7c28691ab86cf01"
            ],
            "layout": "IPY_MODEL_8c160059ece9404083eb892c3778a00e"
          }
        },
        "05d5628b758842268116d4dbd2c79c28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58bcefef24ea4dd6958d6d165bcc0946",
            "placeholder": "​",
            "style": "IPY_MODEL_21b4512319ba4300857b1f28e39b6de0",
            "value": "1M-GPT4-Augmented.parquet: 100%"
          }
        },
        "cfb22b50282e4000a70a74c07726e52f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ffb825002e674fc6810f6a2428eab854",
            "max": 1008442855,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8f95f8dcca03426b8b1467aa67e1b898",
            "value": 1008442855
          }
        },
        "b7eea09116a340bfb7c28691ab86cf01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecdb434942ac4a4bb124cae77b92141c",
            "placeholder": "​",
            "style": "IPY_MODEL_10273425351d4adcb877d730b0c69672",
            "value": " 1.01G/1.01G [00:16&lt;00:00, 50.2MB/s]"
          }
        },
        "8c160059ece9404083eb892c3778a00e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58bcefef24ea4dd6958d6d165bcc0946": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21b4512319ba4300857b1f28e39b6de0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ffb825002e674fc6810f6a2428eab854": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f95f8dcca03426b8b1467aa67e1b898": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ecdb434942ac4a4bb124cae77b92141c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10273425351d4adcb877d730b0c69672": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7a35bfd3b2444e9bf491280c853f3b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d3e01540abef4fbba8e00cb472e38b02",
              "IPY_MODEL_6e504c84457f45eb9d7b0537b83700d5",
              "IPY_MODEL_a4907bb1017a4f4794bde1cab94cd435"
            ],
            "layout": "IPY_MODEL_c137aba7af9c4d06aa6cff93217fe112"
          }
        },
        "d3e01540abef4fbba8e00cb472e38b02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39e972e00cd44ab4a1cf3f32f7918e4b",
            "placeholder": "​",
            "style": "IPY_MODEL_5e55761a83bc48d7a4bc3cfdb1a7dafe",
            "value": "Generating train split: "
          }
        },
        "6e504c84457f45eb9d7b0537b83700d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4d042cdfc1c4bfd98dc5c1365dd4031",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b589c830f232470c886cf85769980157",
            "value": 1
          }
        },
        "a4907bb1017a4f4794bde1cab94cd435": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df2a45413e804d9284a477a1751560dd",
            "placeholder": "​",
            "style": "IPY_MODEL_1eae555773d8422189079b31000af21f",
            "value": " 994896/0 [00:24&lt;00:00, 40439.34 examples/s]"
          }
        },
        "c137aba7af9c4d06aa6cff93217fe112": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39e972e00cd44ab4a1cf3f32f7918e4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e55761a83bc48d7a4bc3cfdb1a7dafe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4d042cdfc1c4bfd98dc5c1365dd4031": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "b589c830f232470c886cf85769980157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df2a45413e804d9284a477a1751560dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1eae555773d8422189079b31000af21f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}